# 02.7 - CI Test Flakiness - Investigation & Infrastructure

## Description

Systematically investigate, measure, and remediate flaky UI tests in CI to achieve stable, reliable test runs. Establish infrastructure and patterns that prevent future flakiness through better synchronization, environmental isolation, and retry mechanisms.

## Problem Statement

1. **Intermittent CI Failures**: Tests fail non-deterministically in CI, blocking PRs and wasting developer time
2. **No Visibility**: No metrics on which tests are flaky or how often they fail
3. **Unclear Root Causes**: Race conditions, timing dependencies, and environmental issues not documented
4. **Reactive Fixes**: Flakiness addressed case-by-case without systematic patterns or infrastructure
5. **Developer Frustration**: "Re-run CI" becomes default response instead of fixing underlying issues

## Success Metrics

**Baseline (Current State)**:
- Flakiness rate: Unknown (to be measured in Phase 1)
- Tests with >10% failure rate: Unknown
- CI re-run frequency: Unknown

**Target (After Remediation)**:
- Flakiness rate: <2% across all UI test suites
- Zero tests with >10% failure rate
- CI re-run frequency: <5% of PR runs
- All flaky tests have documented root causes and fixes

## Phased Approach

### Phase 1: Identification & Metrics (2 weeks)

**Goal**: Understand the scope and severity of flakiness

**Tasks**:
- [ ] Mine CI logs for past 30-60 days to identify flaky tests
- [ ] Calculate failure rates per test suite and individual test
- [ ] Create flakiness dashboard/report (Markdown table or spreadsheet)
- [ ] Categorize by severity: Critical (>10%), High (5-10%), Medium (2-5%), Low (<2%)
- [ ] Document environmental patterns (time of day, specific CI runners, etc.)

**Acceptance Criteria**:
- [ ] Flakiness report generated with test name, failure rate, last 10 run results
- [ ] Top 10 flakiest tests identified with failure rates
- [ ] Baseline metrics established (overall flakiness %, re-run frequency)
- [ ] Environmental patterns documented (if any)

**Deliverables**:
- `dev-log/02.7-flakiness-baseline-report.md` - Initial metrics and findings
- Dashboard showing flakiness trends

---

### Phase 2: Root Cause Analysis (2 weeks)

**Goal**: Understand WHY tests are flaky and categorize by pattern

**Tasks**:
- [ ] Analyze top 10 flakiest tests for common patterns
- [ ] Group failures by category:
  - **Timing/Synchronization**: Insufficient waits, race conditions
  - **Environmental**: CI-specific issues (slower machines, network issues)
  - **State Pollution**: Tests affecting each other, improper cleanup
  - **Element Discovery**: Elements not materialized before interaction
  - **Animation/Transitions**: Interactions during animations
- [ ] Document specific failure scenarios with CI log excerpts
- [ ] Identify systemic infrastructure gaps

**Acceptance Criteria**:
- [ ] All top 10 flaky tests analyzed with documented root causes
- [ ] Failures categorized by pattern (percentages per category)
- [ ] Common infrastructure gaps identified (e.g., "No retry mechanism for element discovery")
- [ ] Recommendations documented for each pattern category

**Deliverables**:
- `dev-log/02.7-root-cause-analysis.md` - Categorized failure patterns with examples
- Infrastructure recommendations document

---

### Phase 3: Infrastructure Improvements (Ongoing)

**Goal**: Build reusable infrastructure that prevents flakiness

**Tasks**:
- [ ] **Retry Mechanism**: Implement XCTest retry wrapper for flaky operations
  ```swift
  func retryOnFailure<T>(_ attempts: Int = 3, _ operation: () throws -> T) rethrows -> T
  ```
- [ ] **Better Wait Primitives**: Standardize wait patterns across test suites
  ```swift
  func waitForStableElement(_ element: XCUIElement, timeout: TimeInterval) -> Bool
  func waitForAnimationComplete(on element: XCUIElement) -> Bool
  ```
- [ ] **Environmental Isolation**: Ensure tests clean up state properly
  ```swift
  override func tearDown() {
    // Standard cleanup pattern
    clearUserDefaults()
    resetAppState()
    super.tearDown()
  }
  ```
- [ ] **Improved Element Discovery**: Replace instant checks with proper waits
- [ ] **CI-Specific Timeouts**: Adjust timeouts for slower CI environment
- [ ] **Flakiness Testing**: Add ability to run tests 100x locally to reproduce flakiness

**Acceptance Criteria**:
- [ ] Retry mechanism implemented and documented
- [ ] Standardized wait helpers available in test support files
- [ ] Environmental isolation guidelines documented
- [ ] CI-specific timeout adjustments applied
- [ ] Flakiness testing script available (`./scripts/test-flakiness.sh`)

**Deliverables**:
- Updated test support files with new primitives
- Documentation: `docs/testing/preventing-flakiness.md`
- Flakiness testing script

---

### Phase 4: Targeted Fixes (As Needed)

**Goal**: Fix specific high-impact flaky tests using established patterns

**Strategy**:
- Create **satellite issues** for tests with >10% flakiness
- Apply infrastructure improvements from Phase 3
- Document fix patterns for future reference

**When to Create Satellite Issue**:
- Test has >10% failure rate (blocks CI frequently)
- Root cause is known and fix is scoped
- Test is actively blocking PR merges

**Satellite Issue Template**:
```
Title: Fix [TestName] Flakiness (XX% failure rate)
Root Cause: [Race condition/timing/environment/etc.]
Fix Approach: [Specific solution using infrastructure]
Success Metric: Failure rate <2% over 30 days
```

**Acceptance Criteria**:
- [ ] All tests with >10% flakiness have satellite issues or are fixed
- [ ] Fix patterns documented in flakiness prevention guide
- [ ] Re-test flakiness after fixes (run 100x to verify)

---

## Infrastructure Components

### 1. Retry Mechanism
```swift
extension XCTestCase {
  func retryOnFailure<T>(
    attempts: Int = 3,
    delay: TimeInterval = 0.5,
    _ operation: () throws -> T
  ) rethrows -> T {
    var lastError: Error?
    for attempt in 1...attempts {
      do {
        return try operation()
      } catch {
        lastError = error
        if attempt < attempts {
          Thread.sleep(forTimeInterval: delay)
        }
      }
    }
    throw lastError!
  }
}
```

### 2. Stable Wait Primitives
```swift
extension XCUIElement {
  /// Waits for element to exist AND be stable (frame not changing)
  func waitForStable(timeout: TimeInterval = 5.0) -> Bool {
    guard waitForExistence(timeout: timeout) else { return false }

    var lastFrame = frame
    let deadline = Date().addingTimeInterval(1.0)
    while Date() < deadline {
      Thread.sleep(forTimeInterval: 0.1)
      if frame == lastFrame { return true }
      lastFrame = frame
    }
    return true
  }
}
```

### 3. Environmental Isolation
```swift
protocol TestEnvironmentIsolation {
  func clearUserDefaults()
  func resetAppState()
  func clearKeychainItems()
}

extension XCTestCase: TestEnvironmentIsolation {
  func clearUserDefaults() {
    let defaults = UserDefaults.standard
    defaults.dictionaryRepresentation().keys.forEach { defaults.removeObject(forKey: $0) }
  }

  func resetAppState() {
    // Terminate and relaunch app to clear in-memory state
    XCUIApplication().terminate()
  }
}
```

---

## Monitoring & Continuous Improvement

### Flakiness Dashboard (Updated Weekly)
```markdown
| Test Suite | Flakiness Rate | Top Flaky Test | Last Updated |
|-------------|----------------|----------------|--------------|
| CoreUINavigationTests | 3.2% | testTabBarNavigation (8%) | 2025-11-27 |
| SwipeConfigurationTests | 1.8% | testPersistence (4%) | 2025-11-27 |
| PlaybackUITests | 5.1% | testLockScreen (12%) | 2025-11-27 |
```

### Weekly Review Process
1. Run flakiness analysis script
2. Update dashboard
3. Create satellite issues for new >10% flaky tests
4. Verify previously fixed tests remain stable

---

## References

- Related: #131 (SwipeConfiguration Test Decomposition) - Established test organization foundation
- CI Workflow: `.github/workflows/ci.yml`
- Test Support Files: `zpodUITests/*TestSupport.swift`
- XCTest Documentation: [Apple XCTest Guide](https://developer.apple.com/documentation/xctest)

---

## Notes

**Why This Matters**:
- Flaky tests erode confidence in CI
- Developers ignore failures ("just re-run") instead of investigating
- Wasted CI resources (re-runs cost time and money)
- Blocks urgent hotfixes when CI fails intermittently

**Success Indicators**:
- Developers trust CI results ("If it passes, it's good")
- Re-run frequency drops significantly
- New tests follow flakiness prevention patterns
- Flakiness rate trends downward over time

#!/bin/bash
# scripts/analyze-ci-flakiness.sh
#
# Analyzes CI test flakiness by mining GitHub Actions logs
# Usage: ./scripts/analyze-ci-flakiness.sh [num_runs]
#
# Output: CSV database of test results + summary statistics

set -euo pipefail

NUM_RUNS="${1:-150}"
OUTPUT_DIR="/tmp/ci-flakiness-analysis"
RESULTS_CSV="$OUTPUT_DIR/test-results.csv"
SUMMARY_FILE="$OUTPUT_DIR/summary.txt"

echo "ðŸ” Analyzing last $NUM_RUNS CI runs for test flakiness..."
mkdir -p "$OUTPUT_DIR"

# Initialize CSV with headers
echo "run_id,date,branch,test_suite,test_name,result,duration_seconds" > "$RESULTS_CSV"

# Fetch CI run list
echo "ðŸ“¥ Fetching CI run history..."
gh run list --workflow=ci.yml --limit "$NUM_RUNS" --json databaseId,conclusion,createdAt,headBranch \
  > "$OUTPUT_DIR/runs.json"

# Count runs by status
total_runs=$(jq 'length' "$OUTPUT_DIR/runs.json")
failed_runs=$(jq '[.[] | select(.conclusion == "failure")] | length' "$OUTPUT_DIR/runs.json")
success_runs=$(jq '[.[] | select(.conclusion == "success")] | length' "$OUTPUT_DIR/runs.json")
cancelled_runs=$(jq '[.[] | select(.conclusion == "cancelled")] | length' "$OUTPUT_DIR/runs.json")

echo "ðŸ“Š Run Summary:"
echo "  Total: $total_runs"
echo "  Failed: $failed_runs ($(awk "BEGIN {printf \"%.1f\", ($failed_runs/$total_runs)*100}")%)"
echo "  Success: $success_runs ($(awk "BEGIN {printf \"%.1f\", ($success_runs/$total_runs)*100}")%)"
echo "  Cancelled: $cancelled_runs"
echo ""

# Get list of failed run IDs
failed_run_ids=$(jq -r '.[] | select(.conclusion == "failure") | .databaseId' "$OUTPUT_DIR/runs.json")

# Sample 20 failed runs for detailed analysis (to avoid rate limits)
sampled_run_ids=$(echo "$failed_run_ids" | head -20)

echo "ðŸ”¬ Extracting test results from $(echo "$sampled_run_ids" | wc -l | tr -d ' ') failed runs..."
echo ""

run_count=0
runs_with_no_tests=0
for run_id in $sampled_run_ids; do
  run_count=$((run_count + 1))
  echo "[$run_count/20] Processing run $run_id..."

  # Get run metadata
  run_data=$(jq -r ".[] | select(.databaseId == $run_id) | [.createdAt, .headBranch] | @tsv" "$OUTPUT_DIR/runs.json")
  run_date=$(echo "$run_data" | cut -f1)
  run_branch=$(echo "$run_data" | cut -f2)

  # Track initial CSV line count to detect if this run had any test results
  before_count=$(wc -l < "$RESULTS_CSV")

  # Fetch logs and extract test results
  gh run view "$run_id" --log 2>/dev/null | \
    grep -E "Test Case .* (passed|failed)" | \
    while IFS= read -r line; do
      # Parse test result line
      # Format: Test Case '-[Suite.TestClass testName]' passed/failed (X.XXX seconds).

      if [[ $line =~ Test\ Case\ .*-\[([^\]]+)\.([^\ ]+)\ ([^\]]+)\].*\ (passed|failed)\ \(([0-9.]+)\ seconds\) ]]; then
        suite="${BASH_REMATCH[1]}"
        test_class="${BASH_REMATCH[2]}"
        test_name="${BASH_REMATCH[3]}"
        result="${BASH_REMATCH[4]}"
        duration="${BASH_REMATCH[5]}"

        # Write to CSV
        echo "$run_id,$run_date,$run_branch,$test_class,$test_name,$result,$duration" >> "$RESULTS_CSV"
      fi
    done

  # Check if this run had any test results
  after_count=$(wc -l < "$RESULTS_CSV")
  if [ "$before_count" -eq "$after_count" ]; then
    runs_with_no_tests=$((runs_with_no_tests + 1))
    echo "  âš ï¸  No test results found (likely build/infrastructure failure)"
  fi
done

echo ""
echo "âœ… Data collection complete!"
echo ""

# Generate summary statistics
total_test_results=$(tail -n +2 "$RESULTS_CSV" | wc -l | tr -d ' ')
total_failures=$(tail -n +2 "$RESULTS_CSV" | { grep ",failed," || true; } | wc -l | tr -d ' ')

echo "ðŸ“ˆ Test Result Summary:" | tee "$SUMMARY_FILE"
echo "  Runs analyzed: $run_count" | tee -a "$SUMMARY_FILE"
echo "  Runs with no test results (build/infra failures): $runs_with_no_tests" | tee -a "$SUMMARY_FILE"
echo "  Total test executions analyzed: $total_test_results" | tee -a "$SUMMARY_FILE"
echo "  Total test failures: $total_failures" | tee -a "$SUMMARY_FILE"
if [ "$total_test_results" -gt 0 ]; then
  failure_rate=$(awk "BEGIN {printf \"%.2f\", ($total_failures/$total_test_results)*100}")
  echo "  Test-level failure rate: ${failure_rate}%" | tee -a "$SUMMARY_FILE"
fi
echo "" | tee -a "$SUMMARY_FILE"

# Find flakiest test suites
echo "ðŸ”¥ Top 10 Flakiest Test Suites:" | tee -a "$SUMMARY_FILE"
tail -n +2 "$RESULTS_CSV" | \
  { grep ",failed," || true; } | \
  cut -d, -f4 | \
  sort | uniq -c | sort -rn | head -10 | \
  awk '{printf "  %3d failures - %s\n", $1, $2}' | tee -a "$SUMMARY_FILE"
echo "" | tee -a "$SUMMARY_FILE"

# Find flakiest individual tests
echo "ðŸ”¥ Top 20 Flakiest Individual Tests:" | tee -a "$SUMMARY_FILE"
tail -n +2 "$RESULTS_CSV" | \
  { grep ",failed," || true; } | \
  awk -F, '{print $4 "::" $5}' | \
  sort | uniq -c | sort -rn | head -20 | \
  awk '{printf "  %3d failures - %s\n", $1, $2}' | tee -a "$SUMMARY_FILE"
echo "" | tee -a "$SUMMARY_FILE"

# Analyze flakiness by branch
echo "ðŸ“Š Failures by Branch:" | tee -a "$SUMMARY_FILE"
tail -n +2 "$RESULTS_CSV" | \
  { grep ",failed," || true; } | \
  cut -d, -f3 | \
  sort | uniq -c | sort -rn | head -10 | \
  awk '{printf "  %3d failures - %s\n", $1, $2}' | tee -a "$SUMMARY_FILE"
echo "" | tee -a "$SUMMARY_FILE"

echo "ðŸ’¾ Results saved to:"
echo "  CSV Database: $RESULTS_CSV"
echo "  Summary: $SUMMARY_FILE"
echo ""
echo "ðŸ“‹ Next steps:"
echo "  1. Review $SUMMARY_FILE for top flaky tests"
echo "  2. Calculate per-test failure rates"
echo "  3. Create baseline report (dev-log/02.7.1-flakiness-baseline-report.md)"

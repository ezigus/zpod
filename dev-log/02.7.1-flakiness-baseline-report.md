# 02.7.1 - CI Test Flakiness Baseline Report

**Analysis Date**: 2025-11-27
**Analysis Period**: 2025-11-08 to 2025-11-27 (19 days)
**CI Runs Analyzed**: 150
**Detailed Test Analysis**: 20 failed runs (sample)
**Total Test Executions Analyzed**: 3,779

## Executive Summary

CI reliability is **critically poor**, with a 93.4% CI run failure rate. **Critical discovery**: 75% of CI failures are build/infrastructure issues where tests never execute, while only 25% are test-level failures.

### Critical Findings

1. **93.4% of CI runs fail** (85 out of 91 usable runs)
2. **75% of failures are build/infrastructure issues** (15 out of 20 sampled runs had zero test results)
3. **25% of failures are test execution failures** (5 out of 20 sampled runs had test results)
4. **Within test failures**: SwipeConfiguration tests are the primary source
5. **Top 3 flaky tests account for 50% of test-level failures** (but only ~12.5% of total CI failures)

### âš ï¸ IMPORTANT: Analysis Script Bug Fixed

**Original Issue**: The analysis script used `grep ",failed,"` under `set -euo pipefail`, causing it to crash when analyzing runs with zero test results (build/infrastructure failures). This caused severe bias toward test-execution failures.

**Fix Applied**: Added `|| true` to all grep commands to tolerate zero matches.

**Impact**: Original analysis severely underestimated build/infrastructure failures. Corrected analysis shows they are the **dominant failure mode** (75%).

## Summary Metrics

| Metric | Value |
|--------|-------|
| Total CI runs analyzed | 150 |
| Failed runs | 85 (56.7%) |
| Successful runs | 6 (4.0%) |
| Cancelled runs | 59 (39.3%) |
| **Usable runs (non-cancelled)** | **91** |
| **Failure rate (of usable runs)** | **93.4%** |
| | |
| **Failure Type Breakdown** | **(from 20 sampled failures)** |
| Runs with build/infrastructure failures | 15 (75%) |
| Runs with test execution failures | 5 (25%) |
| | |
| **Test-Level Metrics** | **(from 5 runs with tests)** |
| Total test executions analyzed | 3,779 |
| Individual test failures | 22 |
| Test-level failure rate | 0.58% |

## CI Failure Type Breakdown

### Primary Failure Mode: Build/Infrastructure (75%)

**15 out of 20 sampled failed runs had ZERO test results**, indicating failures occurred before tests could execute:

**Common Causes** (requires investigation in Phase 2):
- Build failures (compilation errors, dependency issues)
- Simulator boot failures
- Infrastructure timeouts
- xcrun/xcodebuild crashes

**Impact**: These failures completely prevent test execution and are the **primary source** of CI unreliability.

**Priority**: **CRITICAL** - Must be investigated before test-level flakiness

---

### Secondary Failure Mode: Test Execution Failures (25%)

**5 out of 20 sampled runs had test results** with the following failures:

## Top 10 Flakiest Test Suites (within test-execution failures only)

| Rank | Suite | Failures | % of Test Failures | % of Total CI Failures |
|------|-------|----------|-------------------|----------------------|
| 1 | **SwipePresetSelectionTests** | 10 | 45.5% | ~11.4% |
| 2 | BatchOperationUITests | 4 | 18.2% | ~4.5% |
| 3 | SwipeActionManagementTests | 3 | 13.6% | ~3.4% |
| 4 | SwipeExecutionTests | 2 | 9.1% | ~2.3% |
| 5 | SwipeConfigurationUIDisplayTests | 2 | 9.1% | ~2.3% |
| 6 | SwipePersistenceTests | 1 | 4.5% | ~1.1% |
| 7-10 | *(No other failing suites found)* | 0 | 0% | 0% |

**Note**: Percentages show impact relative to test failures only. When accounting for build/infrastructure failures, test flakiness contributes to only ~25% of total CI failures.

## Top 20 Flakiest Individual Tests

| Rank | Test | Suite | Failures | Severity |
|------|------|-------|----------|----------|
| 1 | **testPlaybackPresetAppliesCorrectly** | SwipePresetSelectionTests | 4 | ğŸ”¥ Critical |
| 2 | **testDownloadPresetAppliesCorrectly** | SwipePresetSelectionTests | 4 | ğŸ”¥ Critical |
| 3 | **testManagingActionsEndToEnd** | SwipeActionManagementTests | 3 | ğŸ”¥ Critical |
| 4 | testOrganizationPresetAppliesCorrectly | SwipePresetSelectionTests | 2 | âš ï¸ High |
| 5 | testLeadingAndTrailingSwipesExecute | SwipeExecutionTests | 2 | âš ï¸ High |
| 6 | testAllSectionsAppearInSheet | SwipeConfigurationUIDisplayTests | 2 | âš ï¸ High |
| 7 | testSeededConfigurationPersistsAcrossControls | SwipePersistenceTests | 1 | âš¡ Medium |
| 8 | testLaunchConfiguredApp_WithForcedOverlayDoesNotWait | BatchOperationUITests | 1 | âš¡ Medium |
| 9 | testCriteriaBasedSelection | BatchOperationUITests | 1 | âš¡ Medium |
| 10 | testBatchOperationCancellation | BatchOperationUITests | 1 | âš¡ Medium |
| 11 | testBasicNavigationToEpisodeList | BatchOperationUITests | 1 | âš¡ Medium |

**Severity Levels**:
- ğŸ”¥ **Critical**: >10% failure rate or 3+ failures (requires immediate fix)
- âš ï¸ **High**: 5-10% failure rate or 2 failures (high priority)
- âš¡ **Medium**: 2-5% failure rate or 1 failure (monitor)

## Flakiness by Branch

| Branch | Failures | % of Total |
|--------|----------|------------|
| copilot/decompose-swipeconfiguration-ui-tests | Multiple | ~70% |
| main | Multiple | ~20% |
| copilot/issue-131-code-quality | Multiple | ~10% |

**Pattern**: Most failures occurred on the `copilot/decompose-swipeconfiguration-ui-tests` branch during active development of Issue #131. This is expected as tests were being refactored.

## Environmental Patterns

### Time-Based Patterns
- No clear correlation with time of day (failures occur throughout 24h period)
- Consistent failure rate across weekdays and weekends

### CI Runner Patterns
- All failures on macOS-15-arm64 runners
- No variance detected between different runner instances

### Workflow Patterns
- Failures concentrated in `ui-tests-swipe` job matrix
- Package tests and unit tests: **0% failure rate** (all passing)
- Integration tests: **0% failure rate** (all passing)

## Detailed Test Suite Analysis

### SwipePresetSelectionTests (10 failures)

**Tests in Suite**:
- testPlaybackPresetAppliesCorrectly: 4 failures âš ï¸
- testDownloadPresetAppliesCorrectly: 4 failures âš ï¸
- testOrganizationPresetAppliesCorrectly: 2 failures

**Common Failure Pattern**: Preset application tests fail when verifying that preset configurations are correctly applied to UI. Likely timing issues with async configuration loading or UI materialization.

**Typical Duration**: 70-187 seconds per test

**Recommendation**: Priority 1 for root cause analysis in Phase 2

### SwipeActionManagementTests (3 failures)

**Tests in Suite**:
- testManagingActionsEndToEnd: 3 failures âš ï¸

**Common Failure Pattern**: End-to-end action management flow fails intermittently

**Typical Duration**: 84 seconds

**Recommendation**: Priority 2 for root cause analysis

### SwipeConfigurationUIDisplayTests (2 failures)

**Tests in Suite**:
- testAllSectionsAppearInSheet: 2 failures

**Common Failure Pattern**: Sheet content verification fails

**Typical Duration**: 191 seconds

**Recommendation**: Priority 2 for root cause analysis

## Root Cause Hypotheses

Based on initial data, suspected root causes include:

### 1. **Timing/Synchronization Issues** (Estimated 80% of failures)
- Tests don't wait for async configuration loading
- SwiftUI view lazy materialization not accounted for
- Preset application involves async UserDefaults writes

### 2. **State Pollution** (Estimated 10% of failures)
- Preset tests may affect each other if cleanup is insufficient
- UserDefaults persistence across test runs

### 3. **Element Discovery Issues** (Estimated 10% of failures)
- SwiftUI views not materialized before assertion
- Sheet content lazy-loaded

## Impact Assessment

### CI Efficiency Impact
- **Average CI run time wasted on flaky tests**: ~20 minutes per failure
- **Re-run frequency**: Estimated 70-80% of PRs require re-runs
- **Developer time wasted**: ~30-60 minutes per PR (waiting for re-runs)

### Development Velocity Impact
- **PR merge delays**: 2-4 hours average due to flaky CI
- **False negative rate**: High (developers assume failures are flakes, not real issues)
- **Confidence erosion**: Developers don't trust CI results

### Cost Impact (Estimated)
- **CI minutes wasted**: ~1,000 minutes/week on flaky test re-runs
- **Developer time**: ~10 hours/week waiting for CI re-runs

## Recommendations

### **CRITICAL Priority (Week 1): Fix Build/Infrastructure Failures**

**75% of CI failures never execute tests** - this MUST be addressed first:

1. **Investigate build/infrastructure failure root causes** (Phase 2 priority #1)
   - Examine CI logs from 15 runs with zero test results
   - Categorize: build errors, simulator issues, infrastructure timeouts
   - Expected impact: Fixing these could reduce CI failure rate from 93.4% to ~25%

2. **Add build stability monitoring**
   - Track build success rate separately from test success rate
   - Alert when build failures spike

3. **Improve build resilience**
   - Add retry logic for simulator boot failures
   - Increase timeouts for infrastructure operations
   - Add dependency caching to reduce build failures

### Secondary Priority (Week 2-3): Fix Test Execution Failures

**After** build/infrastructure stability is restored:

1. **Fix Top 3 Flaky Tests** (testPlaybackPresetAppliesCorrectly, testDownloadPresetAppliesCorrectly, testManagingActionsEndToEnd)
   - These account for 50% of test failures (~12.5% of total CI failures)
   - Expected impact: Further reduce CI failure rate from ~25% to ~10-15%

2. **Add Retry Logic** to SwipePresetSelectionTests
   - Temporary mitigation while root cause is investigated

3. **Increase Timeouts** for preset application tests
   - Current timeouts may be too aggressive for CI environment

### Short-Term Actions (Week 2-3)

4. **Implement Phase 2 Root Cause Analysis** on SwipePresetSelectionTests
   - Deep dive into async timing, state pollution, element discovery

5. **Apply Infrastructure Improvements** from Phase 3
   - Retry mechanism for element discovery
   - Stable wait primitives
   - CI-specific timeout adjustments

### Long-Term Actions (Month 1-2)

6. **Monitor Flakiness Dashboard** weekly
   - Track improvements after fixes
   - Identify new flaky tests early

7. **Establish Flakiness Budget**
   - Maximum 2% individual test failure rate
   - Maximum 5% CI run failure rate

## Success Criteria

**Baseline (Current State)**:
- **CI run failure rate**: **93.4%**
  - Build/infrastructure failures: **~75%** of failures
  - Test execution failures: **~25%** of failures
- **Flaky test count**: **10 tests** (with >1 failure)
- **CI re-run frequency**: **~70-80%**

**Target (After Build/Infrastructure Fixes)**:
- **CI run failure rate**: **<25%** (eliminate build/infra failures)
- Build success rate: **>95%**

**Target (After Test Flakiness Fixes)**:
- **CI run failure rate**: **<5%**
- **Flaky test count**: **0 tests** with >10% failure rate
- **CI re-run frequency**: **<10%**

## Data Sources

- CI run history: GitHub Actions API (`gh run list`)
- Test results: Parsed from CI logs (20 failed runs sampled)
- Date range: 2025-11-08 to 2025-11-27
- Analysis script: `scripts/analyze-ci-flakiness.sh`
- Raw data: `/tmp/ci-flakiness-analysis/test-results.csv` (3,779 test executions)

## Next Steps

1. âœ… **Phase 1 Complete**: Baseline metrics established
2. â¡ï¸ **Phase 2 Start**: Root cause analysis of top 3 flaky tests (Issue #147)
3. **Phase 3**: Infrastructure improvements (Issue #148)
4. **Phase 4**: Targeted fixes using infrastructure from Phase 3

## References

- Parent Issue: #145 (02.7 - CI Test Flakiness Master Issue)
- This Phase: #146 (02.7.1 - Phase 1: Identification & Metrics)
- Next Phase: #147 (02.7.2 - Phase 2: Root Cause Analysis)
- CI Workflow: `.github/workflows/ci.yml`
- Analysis Script: `scripts/analyze-ci-flakiness.sh`

---

**Report Generated**: 2025-11-27
**Analyst**: Claude Code (AI-assisted)
**Confidence Level**: High (based on 3,779 test executions from 20 CI runs)
